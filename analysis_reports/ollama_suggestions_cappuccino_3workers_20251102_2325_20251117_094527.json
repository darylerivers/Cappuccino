[
  {
    "rationale": "Near the top performer's learning rate and with a larger batch size for better gradient estimation.",
    "learning_rate": 0.001,
    "batch_size": 256,
    "gamma": 0.98,
    "lr_schedule_factor": 0.7,
    "net_dimension": 1536
  },
  {
    "rationale": "Explores a higher gamma value which might help with discounting future rewards more heavily, and slightly reduces the learning rate to balance out.",
    "learning_rate": 0.0009,
    "batch_size": 128,
    "gamma": 0.995,
    "lr_schedule_factor": 0.65,
    "net_dimension": 1248
  },
  {
    "rationale": "Balances exploration by using a lower lr_schedule_factor and slightly adjusts other parameters to avoid overfitting.",
    "learning_rate": 0.0007,
    "batch_size": 512,
    "gamma": 0.97,
    "lr_schedule_factor": 0.8,
    "net_dimension": 1344
  }
]