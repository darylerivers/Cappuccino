[
  {
    "rationale": "Explores a slightly higher lr around the top performers, increases batch size for more stable updates, and maintains other near-optimal settings.",
    "learning_rate": 0.0001,
    "batch_size": 512,
    "gamma": 0.99,
    "ppo_epochs": 8,
    "net_dimension": 1400,
    "max_grad_norm": 1.5,
    "min_cash_reserve": 0.06,
    "volatility_penalty": 0.02
  },
  {
    "rationale": "Explores a lower lr around the top performers, increases ppo_epochs for more fine-grained updates, and slightly reduces net_dimension to balance complexity.",
    "learning_rate": 5e-05,
    "batch_size": 1024,
    "gamma": 0.98,
    "ppo_epochs": 6,
    "net_dimension": 1200,
    "max_grad_norm": 1.0,
    "min_cash_reserve": 0.07,
    "volatility_penalty": 0.015
  },
  {
    "rationale": "Explores a higher lr around the top performers, decreases batch size for faster updates, and slightly increases time_decay_floor to stabilize learning.",
    "learning_rate": 0.0002,
    "batch_size": 256,
    "gamma": 0.985,
    "ppo_epochs": 10,
    "net_dimension": 1600,
    "max_grad_norm": 1.75,
    "min_cash_reserve": 0.055,
    "time_decay_floor": 0.2
  }
]