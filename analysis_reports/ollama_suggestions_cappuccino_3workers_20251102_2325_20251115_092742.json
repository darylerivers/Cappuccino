[
  {
    "rationale": "Explores a higher learning rate, near the best performers' range, and slightly increases batch size for more stable gradient updates.",
    "learning_rate": 0.0001,
    "batch_size": 512,
    "gamma": 0.99,
    "net_dimension": 1300,
    "min_cash_reserve": 0.1,
    "max_grad_norm": 1.5,
    "ppo_epochs": 8,
    "thread_num": 11,
    "use_lr_schedule": 1
  },
  {
    "rationale": "Reduces the network dimension slightly to potentially reduce complexity, while adjusting other parameters that showed high correlation.",
    "learning_rate": 5e-05,
    "batch_size": 256,
    "gamma": 0.98,
    "net_dimension": 1200,
    "min_cash_reserve": 0.07,
    "max_grad_norm": 1.4,
    "ppo_epochs": 7,
    "thread_num": 9,
    "use_lr_schedule": 0
  },
  {
    "rationale": "Explores a lower learning rate to potentially increase stability, while adjusting other parameters that showed high correlation.",
    "learning_rate": 1e-05,
    "batch_size": 128,
    "gamma": 0.97,
    "net_dimension": 1400,
    "min_cash_reserve": 0.06,
    "max_grad_norm": 1.6,
    "ppo_epochs": 9,
    "thread_num": 13,
    "use_lr_schedule": 1
  }
]