[
  {
    "rationale": "Higher learning rate and gamma can improve convergence, while moderate batch size allows for better generalization.",
    "learning_rate": 0.001,
    "batch_size": 256,
    "gamma": 0.98,
    "max_drawdown_penalty": 0.04,
    "min_cash_reserve": 0.07,
    "ppo_epochs": 10,
    "net_dimension": 1536
  },
  {
    "rationale": "Lowering the learning rate and gamma can make the model more stable, while increasing the batch size allows for better exploitation of data.",
    "learning_rate": 5e-05,
    "batch_size": 1024,
    "gamma": 0.97,
    "max_drawdown_penalty": 0.06,
    "min_cash_reserve": 0.1,
    "ppo_epochs": 8,
    "net_dimension": 1024
  },
  {
    "rationale": "Exploiting the range around top performers, with slightly higher learning rate and gamma for faster convergence.",
    "learning_rate": 0.0002,
    "batch_size": 512,
    "gamma": 0.99,
    "max_drawdown_penalty": 0.05,
    "min_cash_reserve": 0.06,
    "ppo_epochs": 12,
    "net_dimension": 1792
  }
]